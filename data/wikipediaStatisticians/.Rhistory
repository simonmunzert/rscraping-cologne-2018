pop * 100
# check precision
kwic(fbcorpus, pattern = 'undemocratic') # sounds good
kwic(fbcorpus, pattern = 'ruling*') # probably not
# check recall
kwic(fbcorpus, pattern = 'unaccountable')
kwic(fbcorpus, pattern = 'dodging')
fbdfm <- dfm(fbcorpus, groups = c("type", "party"))
# turn word counts into proportions
fbdfm <- dfm_weight(fbdfm, scheme="prop")
# find % of words in populism dictionary
pop <- dfm_lookup(fbdfm, dictionary = populist_dict)
pop * 100
liwc <- read.csv("../data/liwc-dictionary.csv", stringsAsFactors = FALSE)
anger.words <- liwc$word[liwc$class=="anger"]
sample(anger.words, 10)
anger <- dfm_lookup(fbdfm, dictionary = dictionary(list('anger'=anger.words)))
anger*100
# create corpus
fbcorpus <- corpus(uk)
fbdfm <- dfm(fbcorpus, groups = "party")
# normalize for document length: turn word counts into proportions
fbdfm <- dfm_weight(fbdfm, scheme="prop")
anger <- dfm_lookup(fbdfm, dictionary = dictionary(list('anger'=anger.words)))
anger*100
class(liwc)
liwc$word[liwc$class == "anger"]
anger_words <- liwc$word[liwc$class == "anger"]
sample(anger_words, 10)
anger <- dfm_lookup(fbdfm, dictionary = dictionary(list('anger' = anger_words)))
anger*100
tweets <- read.csv("../data/UK-tweets.csv", stringsAsFactors=F)
tweets$engaging <- ifelse(tweets$communication=="engaging", 1, 0)
tweets <- tweets[!is.na(tweets$engaging),]
head(tweets)
# substitute handles with @ to avoid overfitting
tweets$text <- gsub('@[0-9_A-Za-z]+', '@', tweets$text)
# substitute handles with @ to avoid overfitting
tweets$text <- gsub('@[0-9_A-Za-z]+', '@', tweets$text)
# further preprocessing
twcorpus <- corpus(tweets$text)
summary(twcorpus)
# keep only tokens that appear in 2 or more tweets
# keep punctuation -- it turns out it can be quite informative.
twdfm <- dfm(twcorpus, remove=stopwords("english"), remove_url=TRUE,
ngrams=1:2, verbose=TRUE) #
twdfm <- dfm_trim(twdfm, min_docfreq = 2, verbose=TRUE)
?remove_twitter
?removePunctuation
?remove_punct
?tokens
# split into training and test set
set.seed(123)
training <- sample(1:nrow(tweets), floor(.80 * nrow(tweets)))
test <- (1:nrow(tweets))[1:nrow(tweets) %in% training == FALSE]
# ridge regression
library(glmnet)
require(doMC)
install.packages("glmnet")
source("packages.r")
# ridge regression
registerDoMC(cores=3) # parallel computing on multiple cores using the doMC package
?registerDoMC
options("cores")
detectCores()
# ridge regression
parallel::detectCores()
?cv.glmnet
twdfm[training,]
tweets$engaging[training]
ridge <- cv.glmnet(twdfm[training,], # x matrix
tweets$engaging[training],  # y response
family = "binomial", # family
alpha = 0,
nfolds = 5, # k-folds cross-validation
parallel = TRUE, # enable parallel computing
intercept = TRUE, #
type.measure = "class")
plot(ridge)
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# computing predicted values
preds <- predict(ridge, twdfm[test,], type="class")
table(preds, tweets$engaging[test]) # confusion matrix
# performance metrics
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
# from the different values of lambda, let's pick the highest one that is
# within one standard error of the best one (why? see "one-standard-error"
# rule -- maximizes parsimony)
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]
head(beta)
ridge$lambda
ridge$lambda.1se
summary(ridge)
which(ridge$lambda==ridge$lambda.1se)
# from the different values of lambda, let's pick the highest one that is within one standard error of the best one
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]
beta
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors=F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
df <- df[order(df$coef, decreasing=TRUE),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# lasso regression
lasso <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 1, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[,best.lambda]
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
# elastic net
enet <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 0.5, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
# note: this will not cross-validate across values of alpha
# computing predicted values
preds <- predict(enet, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
source("packages.r")
# converting matrix object
X <- as(twdfm, "dgCMatrix")
# parameters to explore
tryEta <- c(1,2)
tryDepths <- c(1,2,4)
# placeholders for now
bestEta = NA
bestDepth = NA
bestAcc = 0
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold=5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
cat("Best model has eta=",bestEta," and depth=", bestDepth, " : ",
bestAcc," accuracy.\n",sep="")
# running best model
rf <- xgboost(data = X[training,],
label = tweets$engaging[training],
max.depth = bestDepth,
eta = bestEta,
nthread = 4,
nround = 1000,
print_every_n=100L,
objective = "binary:logistic")
# out-of-sample accuracy
preds <- predict(rf, X[test,])
cat("\nAccuracy on test set=", round(accuracy(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(1) on test set=", round(precision(preds>.50, tweets$engaging[test]),3))
cat("\nRecall(1) on test set=", round(recall(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(0) on test set=", round(precision(preds<.50, tweets$engaging[test]==0),3))
cat("\nRecall(0) on test set=", round(recall(preds<.50, tweets$engaging[test]==0),3))
# feature importance
labels <- dimnames(X)[[2]]
importance <- xgb.importance(labels, model = rf, data=X, label=tweets$engaging)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
head(importance, n=20)
# adding sign
sums <- list()
for (v in 0:1){
sums[[v+1]] <- colSums(X[tweets[,"engaging"]==v,])
}
sums <- do.call(cbind, sums)
sign <- apply(sums, 1, which.max)
df <- data.frame(
Feature = labels,
sign = sign-1,
stringsAsFactors=F)
importance <- merge(importance, df, by="Feature")
## best predictors
for (v in 0:1){
cat("\n\n")
cat("value==", v)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
print(head(importance[importance$sign==v,], n=50))
cat("\n")
cat(paste(unique(head(importance$Feature[importance$sign==v], n=50)), collapse=", "))
}
source("packages.r")
source("packages.r")
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://google.de")
parsed_doc
parsed_doc <- read_html("https://facebook.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://www.google.com")
parsed_doc
class(parsed_doc)
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc
# parse with read_html
parsed_doc <- read_html("https://www.google.com")
parsed_doc
Sys.getlocale()
Sys.setlocale("LC_CTYPE", "en_US.UTF-8")
parsed_doc <- read_html("https://www.google.com")
parsed_doc
Sys.setlocale("LC_ALL", "English")
parsed_doc <- read_html("https://www.google.com")
parsed_doc
# inspect parsed object
class(parsed_doc)
html_structure(parsed_doc)
as_list(parsed_doc)
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# relative paths
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
# wildcard (for ONE node)
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
# sibling
html_nodes(parsed_doc, xpath = "//p/preceding-sibling::h1")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# numeric
html_nodes(parsed_doc, xpath = "//div/p[1]")
html_nodes(parsed_doc, xpath =  "//div/p[last()]")
html_nodes(parsed_doc, xpath = "//div[count(.//a)>0]")
html_nodes(parsed_doc, xpath = "//div[count(./@*)>2]")
html_nodes(parsed_doc, xpath = "//*[string-length(text())>50]")
# text-based
html_nodes(parsed_doc, xpath = "//div[@date='October/2011']")
html_nodes(parsed_doc, xpath = "//*[contains(text(), 'magic')]")
html_nodes(parsed_doc, xpath = "//div[starts-with(./@id, 'R')]")
html_nodes(parsed_doc, xpath = "//div[substring-after(./@date, '/')='2003']//i")
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
## preparations -----------------------
source("packages.r")
## preparations -----------------------
source("packages.r")
browseURL("http://www.biermap24.de/brauereiliste.php")
# Chrome:
# 1. right click on page
# 2. select "view source"
# Firefox:
# 1. right click on page
# 2. select "view source"
# Microsoft Edge:
# 1. right click on page
# 2. select "view source"
browseURL("http://www.biermap24.de/brauereiliste.php")
# set temporary working directory
tempwd <- ("../data/breweriesGermany")
dir.create(tempwd)
setwd(tempwd)
browseURL(url)
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
browseURL(url)
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[2]")
anchors
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
cities
cities <- cities[str_detect(cities, "^[[:upper:]]+.")]
cities
head(cities)
cities <- cities[6:length(cities)]
head(cities)
tail(cities)
length(cities)
length(unique(cities))
sort(table(cities))
geocode("Washington")
geocode("Washington")
geocode("Washington")
geocode("Washington")
geocode("Washington")
geocode("Washington")
geocode("Washington")
geocode("Washington")
geocode("Washington")
geocode("Washington")
oding takes a while -> save results in local cache file
# 2500 requests allowed per day
if ( !file.exists("breweries_geo.RData")){
pos <- geocode(cities)
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
head(pos)
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
plot(pos$lon, pos$lat)
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
## return to base working drive
setwd("../../code")
# set temporary working directory
tempwd <- ("../data/wikipediaStatisticians")
dir.create(tempwd)
setwd(tempwd)
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
## step 2: retrieve links
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links
links <- links[!is.na(links)]
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
links
length(links)
##  step 3: extract names
names <- links %>% basename %>% sapply(., URLdecode)  %>% str_replace_all("_", " ") %>% str_replace_all(" \\(.*\\)", "") %>% str_trim
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 5: identify links between statisticians
# loop preparation
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//p//a"), # note: only look for links in p sections; otherwise too many links collected
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks]
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i-1, length(links_in_pslinks)), # -1 for zero-indexing
to=links_in_pslinks-1 # here too
)
)
}
# results
names(connections) <- c("from", "to")
head(connections)
# make symmetrical
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
connections <- connections[!duplicated(connections),]
ze connections
connections$value <- 1
nodesDF <- data.frame(name = names, group = 1)
network_out <- forceNetwork(Links = connections, Nodes = nodesDF, Source = "from", Target = "to", Value = "value", NodeID = "name", Group = "group", zoom = TRUE, fontSize = 14, opacityNoHover = 3)
saveNetwork(network_out, file = 'connections.html')
browseURL("connections.html")
## step 7: identify top nodes in data frame
nodesDF$id <- as.numeric(rownames(nodesDF)) - 1
connections_df <- merge(connections, nodesDF, by.x = "to", by.y = "id", all = TRUE)
to_count_df <- count(connections_df, name)
arrange(to_count_df, desc(n))
